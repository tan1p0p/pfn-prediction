{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hassaku/.pyenv/versions/3.7.0/lib/python3.7/site-packages/chainer/_environment_check.py:37: UserWarning: Accelerate has been detected as a NumPy backend library.\n",
      "vecLib, which is a part of Accelerate, is known not to work correctly with Chainer.\n",
      "We recommend using other BLAS libraries such as OpenBLAS.\n",
      "For details of the issue, please see\n",
      "https://docs.chainer.org/en/stable/tips.html#mnist-example-does-not-converge-in-cpu-mode-on-mac-os-x.\n",
      "\n",
      "Please be aware that Mac OS X is not an officially supported OS.\n",
      "\n",
      "  ''')  # NOQA\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "from chainer.datasets import TupleDataset, split_dataset_random\n",
    "from chainer.iterators import SerialIterator\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, annotation_file_path):\n",
    "        self.height = 224\n",
    "        self.width = 224\n",
    "        self.channel = 3\n",
    "        self.label_num = 16\n",
    "        self.annotation_file_path = annotation_file_path\n",
    "        self.annotation_file = json.load(open((self.annotation_file_path), 'r'))\n",
    "        self.image_list = self.annotation_file['fileList']\n",
    "        self.label_list = self.annotation_file['posList']\n",
    "\n",
    "    def load_data(self):\n",
    "        x = np.zeros((len(self.image_list), self.channel, self.height, self.width), dtype='float32')\n",
    "        for i, imagepath in enumerate(self.image_list):\n",
    "            image = Image.open(imagepath).convert('RGB')\n",
    "            np_image = np.asarray(image.resize((self.height, self.width)))\n",
    "            x[i] = np.moveaxis(np_image, 2, 0)\n",
    "        self.x = x\n",
    "\n",
    "        t = np.zeros((len(self.label_list), self.label_num, self.height, self.width), dtype='float32')\n",
    "        for label_idx, label in enumerate(self.label_list):\n",
    "            for point_idx, point in enumerate(label):\n",
    "                x_pos = int(point[0] / 600 * 56)\n",
    "                y_pos = int(point[1] / 600 * 56)\n",
    "                t[label_idx][point_idx][y_pos][x_pos] = 1\n",
    "        self.t = t\n",
    "\n",
    "        self.dataset = TupleDataset(x, t)\n",
    "        return self.dataset\n",
    "        \n",
    "    def split(self, ratio = (8, 1, 1)):\n",
    "        train_val_rate = (ratio[0] + ratio[1]) / sum(ratio)\n",
    "        train_rate = ratio[0] / (ratio[0] + ratio[1])\n",
    "        train_val, test = split_dataset_random(self.dataset, int(len(self.dataset) * train_val_rate), seed=0)\n",
    "        train, valid = split_dataset_random(train_val, int(len(train_val) * train_rate), seed=0)\n",
    "        self.train = train\n",
    "        self.valid = valid\n",
    "        self.test = test\n",
    "        return (self.train, self.valid, self.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader('../data/raw/hands/hand_position.json')\n",
    "dataloader.load_data()\n",
    "train, valid, test = dataloader.split((8, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "class network():\n",
    "    def __init__(self):\n",
    "        self.base = L.VGG16Layers()\n",
    "        self.dc1 = L.Deconvolution2D(512, 128) # in 7x7, out 14x14\n",
    "        self.dc2 = L.Deconvolution2D(128, 32) # in 14x14, out 28x28\n",
    "        self.dc3 = L.Deconvolution2D(32, 16) # in 28x28, out 56x56\n",
    "        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        h = self.base(x, layers=['pool5'])\n",
    "        h = self.dc1(h['pool5'])\n",
    "        h = self.dc2(h)\n",
    "        h = self.dc3(h)\n",
    "        print(h.shape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu_id = 0  # 使用する GPU 番号\n",
    "n_batch = 2  # バッチサイズ\n",
    "n_epoch = 50  # エポック数\n",
    "\n",
    "# ネットワークを GPU メモリ上に転送\n",
    "# net.to_gpu(gpu_id)\n",
    "\n",
    "# ログ\n",
    "results_train, results_valid = {}, {}\n",
    "results_train['loss'], results_train['accuracy'] = [], []\n",
    "results_valid['loss'], results_valid['accuracy'] = [], []\n",
    "\n",
    "count = 1\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "# ログの保存用\n",
    "results_train = {\n",
    "    'loss': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "results_valid = {\n",
    "    'loss': [],\n",
    "    'accuracy': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chainer.iterators import SerialIterator\n",
    "train_iter = SerialIterator(train, batch_size=4, repeat=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc y_train.\n",
      "(4, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "from chainer.dataset import concat_examples\n",
    "net = network()\n",
    "for epoch in range(n_epoch):\n",
    "    while True:\n",
    "        # ミニバッチの取得\n",
    "        train_batch = train_iter.next()\n",
    "        # x と t に分割\n",
    "        # データを GPU に転送するために、concat_examples に gpu_id を渡す\n",
    "        x_train, t_train = concat_examples(train_batch) #, gpu_id)\n",
    "\n",
    "        # 予測値と目的関数の計算\n",
    "        print('Calc y_train.')\n",
    "        print(x_train.shape)\n",
    "        y_train = net(x_train)\n",
    "        print(y_train.shape)\n",
    "        loss_train = F.mean_squared_error(y_train, t_train)\n",
    "        acc_train = F.accuracy(y_train, t_train)\n",
    "\n",
    "        # 勾配の初期化と勾配の計算\n",
    "        net.cleargrads()\n",
    "        loss_train.backward()\n",
    "\n",
    "        # パラメータの更新\n",
    "        optimizer.update()\n",
    "\n",
    "        # カウントアップ\n",
    "        count += 1\n",
    "\n",
    "        # 1エポック終えたら、valid データで評価する\n",
    "        if train_iter.is_new_epoch:\n",
    "            print('epoch', count, 'end.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
